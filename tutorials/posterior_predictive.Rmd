---
title: "Posterior Predictive Checks"
output:
  html_document:
    df_print: paged
---

The idea behind predictive checking is simple. If a model is a good fit, we should be able to generate data that looks like actual data. To generate the data used for posterior predictive checks, we simulate from either a prior or *posterior predictive distribution*.

Recall that our goal in probabilistic modeling is to build a faithful model to the underlying data generating process. The data we used to train our model was only a finite sample from that process. It could, of course, generate more data. Think of a predictive distribution as the distribution of the observed variables generated by our model. Suppose we generate from the model before we train it on data. In that case, we see samples from the *prior predictive distribution*, which encapsulate our beliefs about what the data will look like before seeing it. Once we see a bit of data and use it to train our model, we get the *posterior predictive distribution*, which gives us updated beliefs about what we will see in future data.

### Focus on a single unseen data point

We usually discuss predictive distributions in terms of an individual unseen data point denoted $\tilde{x}$. We write the prior predictive distribution as: $$\pi(\tilde{x}) = \int \pi(\tilde{x}|\theta) \pi(\theta) d\theta$$

$\pi(\tilde{x}|\theta)$ is the likelihood of the unseen data point.  We integrate this distribution over the prior to obtain $\pi(\tilde{x})$.  This distribution is the same as the [marginal likelihood](https://en.wikipedia.org/wiki/Marginal_likelihood) but evaluated for a single data point.

We write the posterior predictive distribution as: $$\pi(\tilde{x}|X)=\int \pi(\tilde{x}|\theta)\pi(\theta|X)d\theta$$ where $X$ is the data vector we train the model on and $\pi(\theta|X)$ is the posterior distribution function.

## Generating from the prior predictive distribution

The simplest way to generate from the prior predictive is to write out the generative model directly in R, Python, or whichever language is your primary development environment.

Above, I just defined predictive distributions in terms of an individual data point.  In practice, we sample and do inference many data points from these distributions.

For a given value of the parameter θ, we sample many values of $\tilde{x}$ so we can get a sample-based representation of the predictive distribution.

```{r libaries, message=FALSE}
library(tidyverse)
library(tidybayes)
```

```{r load_data, results='hide', message=FALSE}
model_data <- read_delim("https://raw.githubusercontent.com/altdeep/probmodeler/main/tutorials/data/height_weight_age_male.csv", delim = ";") %>%
  filter(age >= 18) %>%
  select(height, weight) %>%
  compose_data
```

```{r}
# Simulate 500 versions of the parameters
num_samples <- 500
n <- 352

a <- rnorm(num_samples, 115, 5)
b <- rlnorm(num_samples, 0, 1)
sigma <- runif(num_samples, 0, 50)

height_prior_pred_samples <- matrix(
    0,
    nrow = num_samples,
    ncol = n
)

for(i in 1:num_samples){
    weight <- rnorm(n, 45, 7)
    mu <- a[i] + b[i] * weight
    height_prior_pred_samples[i, ] <- rnorm(n, mu, sigma[i])
}
```

The sampling process above generated 500 samples of `height_pred` vectors of size `n`=352.  So for each of 500 samples of $a$, $b$, and $\sigma$, we get a height vector of length 352.  In other words, we get a 500 by 352 matrix.

How do we visualize that?  A common approach is to pass each $\tilde{x}$-vector into a density curve estimator.  The resulting density curve function is a summary of the $\tilde{x}$-vector.  So for each value of θ, we get a single curve.

Finally, we just overlay each curve in a visualization.

```{r prior_curves}
plot(
    density(height_prior_pred_samples[, 1]),
    ylim=c(0, .15),
    xlim=c(100, 190),
    main="Prior Predictive Distribution of Height",
    col="#8F272755",
    lwd=.1
)
for(i in 2:model_data$n){
    lines(
        density(height_prior_pred_samples[, i]),
        col="#8F272755"
    )
}

```

## Generating from the posterior predictive in Stan

We use the `generated quantities` block in Stan to do posterior predictive checks. 

```
vector[n] height_pred;
for (i in 1:n) {
   height_pred[i] = normal_rng(mu[i], sigma);
}
```

Here we declare a predictive height variable of length `n`, the same size as our height variable in the data.  Then we sample that variables values using the same likelihood that `height` has in the `model block` (normal distribution) and our `mu` and `sigma` parameters.

```{r stan-libraries, message=FALSE, results='hide'}
library(rstan)
```

```{r , results="hide"}
with_data_code <- '
data {
  int n;
  vector[n] height;
  vector[n] weight;
}
parameters {
  real<lower=0,upper=50> sigma;
  real<lower=0> b;
  real a;
}
transformed parameters {
  vector[n] mu;
  mu = a + b * weight;
}
model {
  a ~ normal(115, 5);
  b ~ lognormal(0, 1); # half-normal because b has 0 bound
  sigma ~ uniform(0, 50);
  weight ~ normal(45, 7);
  height ~ normal(mu, sigma);
}
generated quantities {
  vector[n] height_pred;
  for (i in 1:n) {
    height_pred[i] = normal_rng(mu[i], sigma);
  }
}
'
program_with_data <- stan_model(model_code = with_data_code)
posterior_samples <- sampling(object = program_with_data, data=model_data)
```

In the following plot, we again visualize density curve estimators of $\pi(\tilde{x}|X)$.  The red curves illustrate the prior predictive distribution, the green curves illustrate the posterior predictive distribution, and the black curve is the density curve fit on the actual data.

```{r}
height_post_pred_samples <- rstan::extract(posterior_samples)['height_pred']$height_pred

plot(
    density(height_prior_pred_samples[, 1]),
    ylim=c(0, .1),
    xlim=c(100, 220),
    main="Predictive Distributions of Height",
    col="#8F272755",
    lwd=.1
)
for(i in 2:model_data$n){
    lines(
        density(height_prior_pred_samples[, i]),
        col="#8F272755",
        lwd=.1
    )
}
for(i in 1:model_data$n){
    lines(
        density(height_post_pred_samples[, i]),
        col="#00FF0080",
        lwd=.1
    )
}
lines(
    density(model_data$height),
    col="black"
)
```
